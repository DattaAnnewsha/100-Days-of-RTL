{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DattaAnnewsha/100-Days-of-RTL/blob/main/EE599_hw3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PulVw_da4GML"
      },
      "source": [
        "# HW3 - EE599 Systems for Machine Learning, Fall 2023\n",
        "University of Southern California\n",
        "\n",
        "Instructors: Arash Saifhashemi, Murali Annavaram\n",
        "\n",
        "In this homework assignment, we will ask you to use various methods to implement convolution operation, and then measure and analyze the performance of each method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aG26EClp5nEq"
      },
      "source": [
        "## Prepare your Google Drive\n",
        "- Download `ML_Systems_HW3` zip file from GitHub and unzip the it (you may need to rename the unzipped folder).\n",
        "- Upload unzipped folder to ``My Drive`` in Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02VQPV6q4Cav",
        "outputId": "328a0511-e71f-4a9e-8136-c2575d95ca5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "os.chdir('/content/drive/MyDrive/ml-systems-hw3-DattaAnnewsha-master/src')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITQeV_DO755_"
      },
      "source": [
        "## Verify that you are in the correct working directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2Y1rLNv7434",
        "outputId": "94ea37cd-e0df-4707-912b-1473a456082a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/ml-systems-hw3-DattaAnnewsha-master/src\n"
          ]
        }
      ],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dioEAKJm6BUN"
      },
      "source": [
        "## Create a folder named `build` under `ML_Systems_HW3/src`, which will be used to store executable files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XemyfZFZ6BUN",
        "outputId": "859f4d3d-0c43-4c38-9ef2-1c505f73f5fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘build’: File exists\n"
          ]
        }
      ],
      "source": [
        "!mkdir build"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TcEAwSWG6wSi",
        "outputId": "6dcc2451-9451-4393-a354-b44db8a1207b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Compiled q1_conv2d_naive1.cc successfully!\n",
            "Compiled q2_conv2d_toeplitz1.cc successfully!\n",
            "Compiled q3_conv2d_toeplitz_transB1.cc successfully!\n",
            "Compiled q4_conv2d_toeplitz_avx1.cc successfully!\n",
            "Compiled q5_conv2d_toeplitz_avx_openmp1.cc successfully!\n",
            "Compiled q6_conv2d_toeplitz_blas1.cc successfully!\n",
            "All files compiled successfully!\n"
          ]
        }
      ],
      "source": [
        "!bash compile.csh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0k8aaYu6zXv",
        "outputId": "3050e7f5-f13f-47e6-adab-6f40d2ffcd35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running q1_conv2d_naive1 ...\n",
            "Manual Memory Usage (KB): 16706\n",
            "Virtual Memory (KB): 16716\n",
            "temp: 0\n",
            "Time taken by convolution: 22887 milliseconds\n",
            "Sum of all outputs: 47243571200\n",
            "Input freed\n",
            "Weight freed\n",
            "Output freed\n",
            "-------------------------------------------\n",
            "Running q2_conv2d_toeplitz1 ...\n",
            "Virtual Memory (KB): 79964\n",
            "temp: 0\n",
            "Time taken by function: 13475 milliseconds\n",
            "Sum of all outputs: 47243571200\n",
            "-------------------------------------------\n",
            "Running q3_conv2d_toeplitz_transB1 ...\n",
            "Virtual Memory (KB): 79964\n",
            "temp: 0\n",
            "Time taken by function: 9859 milliseconds\n",
            "Sum of all outputs: 47243571200\n",
            "-------------------------------------------\n",
            "Running q4_conv2d_toeplitz_avx1 ...\n",
            "Virtual Memory (KB): 79964\n",
            "temp: 0\n",
            "Time taken by function: 2308 milliseconds\n",
            "Sum of all outputs: 47243571200\n",
            "-------------------------------------------\n",
            "Running q5_conv2d_toeplitz_avx_openmp1 ...\n",
            "Virtual Memory (KB): 79964\n",
            "temp: 0\n",
            "Time taken by function with 2 threads: 1588 milliseconds\n",
            "Sum of all outputs: 47243571200\n",
            "Time taken by function with 4 threads: 1590 milliseconds\n",
            "Sum of all outputs: 47243571200\n",
            "Time taken by function with 8 threads: 1595 milliseconds\n",
            "Sum of all outputs: 47243571200\n",
            "-------------------------------------------\n",
            "Running q6_conv2d_toeplitz_blas1 ...\n",
            "Virtual Memory (KB): 79964\n",
            "temp: 0\n",
            "Time taken by function: 63 milliseconds\n",
            "Sum of all outputs: 47243571200\n",
            "-------------------------------------------\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "!bash run.csh\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4Jeis0Q6BUO"
      },
      "source": [
        "## Compile and run your code:\n",
        "* To compile your C++ code, run command `!bash compile.csh`\n",
        "* To run your executable code, run command `!bash run.csh`\n",
        "* Check those csh files and modify accordingly when testing your code.\n",
        "* You can write and test your code on your local machine but make sure you can compile and run them on Colab. Also you should report the performance measured from Colab enverionment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNu5C22Q6BUO"
      },
      "source": [
        "## Write code and answer all questions in this notebook.\n",
        "Note that to measure the performance of each code, we want you to flush your cache. All the template files provide code that flush caches, but we need you to find the cache size of your system. Use the command below to display information about your CPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "5OzO0J8J6BUO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "511f1a0c-a131-48b3-b0bb-9c8a3aa1051a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Architecture:            x86_64\n",
            "  CPU op-mode(s):        32-bit, 64-bit\n",
            "  Address sizes:         48 bits physical, 48 bits virtual\n",
            "  Byte Order:            Little Endian\n",
            "CPU(s):                  2\n",
            "  On-line CPU(s) list:   0,1\n",
            "Vendor ID:               AuthenticAMD\n",
            "  Model name:            AMD EPYC 7B12\n",
            "    CPU family:          23\n",
            "    Model:               49\n",
            "    Thread(s) per core:  2\n",
            "    Core(s) per socket:  1\n",
            "    Socket(s):           1\n",
            "    Stepping:            0\n",
            "    BogoMIPS:            4499.99\n",
            "    Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mc\n",
            "                         a cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall n\n",
            "                         x mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_go\n",
            "                         od nopl nonstop_tsc cpuid extd_apicid tsc_known_freq pn\n",
            "                         i pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt a\n",
            "                         es xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy \n",
            "                         cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw top\n",
            "                         oext ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust b\n",
            "                         mi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_\n",
            "                         ni xsaveopt xsavec xgetbv1 clzero xsaveerptr arat npt n\n",
            "                         rip_save umip rdpid\n",
            "Virtualization features: \n",
            "  Hypervisor vendor:     KVM\n",
            "  Virtualization type:   full\n",
            "Caches (sum of all):     \n",
            "  L1d:                   32 KiB (1 instance)\n",
            "  L1i:                   32 KiB (1 instance)\n",
            "  L2:                    512 KiB (1 instance)\n",
            "  L3:                    16 MiB (1 instance)\n",
            "NUMA:                    \n",
            "  NUMA node(s):          1\n",
            "  NUMA node0 CPU(s):     0,1\n",
            "Vulnerabilities:         \n",
            "  Itlb multihit:         Not affected\n",
            "  L1tf:                  Not affected\n",
            "  Mds:                   Not affected\n",
            "  Meltdown:              Not affected\n",
            "  Mmio stale data:       Not affected\n",
            "  Retbleed:              Vulnerable\n",
            "  Spec store bypass:     Vulnerable\n",
            "  Spectre v1:            Vulnerable: __user pointer sanitization and usercopy ba\n",
            "                         rriers only; no swapgs barriers\n",
            "  Spectre v2:            Vulnerable, IBPB: disabled, STIBP: disabled, PBRSB-eIBR\n",
            "                         S: Not affected\n",
            "  Srbds:                 Not affected\n",
            "  Tsx async abort:       Not affected\n"
          ]
        }
      ],
      "source": [
        "!lscpu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Hk0XSjP9_n3"
      },
      "source": [
        "## Q1\n",
        "Implement 2D Convolution in the ``q1_conv2d_naive.cc`` using nested for loops. Assume batch size = 1, no padding and stride = 1. Check `util.h` file and understand what each function does. Use the micro `INDEX_4D_TO_1D` to help convert 4d index to 1d index. Measure and report runtime  (in milliseconds) and memory usage (in KB). Manually calculate memory usage and report. Does your calculation match with the measurement?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Buffer Size:**\n",
        "\n",
        "Input Size = 128 * 128 * 128\n",
        "\n",
        "Weight Size = 128 * 128 * 3 * 3\n",
        "\n",
        "Output Size = 128 * 126 * 126\n",
        "\n",
        "**Memory usage = 16706 KB**\n",
        "\n",
        "\n",
        "Note: for floationg point operation 4bytes * buffer size"
      ],
      "metadata": {
        "id": "D7SPbH9l5jSI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#include <chrono>\n",
        "#include <iostream>\n",
        "\n",
        "#include \"../utils.h\"\n",
        "\n",
        "using namespace std;\n",
        "using namespace std::chrono;\n",
        "\n",
        "int main(void) {\n",
        "  // Modify the cachesize to match your system configuration in Google Colab\n",
        "  const unsigned int cacheSize = 55 * 1024 * 1024;\n",
        "  const unsigned int cacheFloatSize = cacheSize / sizeof(float);\n",
        "\n",
        "  float* cacheFlush = allocateAlignedFloatArray(cacheSize);\n",
        "  bool DBG = false;\n",
        "  unsigned int weight_o;\n",
        "  unsigned int weight_i;\n",
        "  unsigned int weight_r;\n",
        "  unsigned int weight_s;\n",
        "\n",
        "  // Dimension of the convolution input\n",
        "  unsigned int input_c;\n",
        "  unsigned int input_h;\n",
        "  unsigned int input_w;\n",
        "\n",
        "  // Dimension of the convolution weight\n",
        "  if (DBG == false) {\n",
        "    weight_o = 128;\n",
        "    weight_i = 128;\n",
        "    weight_r = 3;\n",
        "    weight_s = 3;\n",
        "\n",
        "    // Dimension of the convolution input\n",
        "    input_c = 128;\n",
        "    input_h = 128;\n",
        "    input_w = 128;\n",
        "  } else {\n",
        "    weight_o = 3;\n",
        "    weight_i = 3;\n",
        "    weight_r = 2;\n",
        "    weight_s = 2;\n",
        "\n",
        "    // Dimension of the convolution input\n",
        "    input_c = 1;\n",
        "    input_h = 3;\n",
        "    input_w = 3;\n",
        "  }\n",
        "\n",
        "  // Declare variables to store the initial memory usage values.\n",
        "  double initialVirtualMemoryUsage, initialResidentSetSize;\n",
        "\n",
        "  // Declare variables to store any subsequent memory usage values (to be used\n",
        "  // later if needed).\n",
        "  double subsequentVirtualMemoryUsage, subsequentResidentSetSize;\n",
        "\n",
        "  // Call the function to get the current memory usage and store the values in\n",
        "  // the initial variables.\n",
        "  getMemoryUsage(initialVirtualMemoryUsage, initialResidentSetSize);\n",
        "\n",
        "  // Allocate input buffers\n",
        "  // Calculate the sizes needed for the arrays\n",
        "  const unsigned int weightSize = weight_o * weight_i * weight_r * weight_s;\n",
        "  const unsigned int inputSize = input_c * input_h * input_w;\n",
        "\n",
        "  // Allocate memory for the arrays using the provided functions\n",
        "  float* weight = allocateAlignedFloatArray(weightSize);\n",
        "  float* input = allocateAlignedFloatArray(inputSize);\n",
        "\n",
        "  // Initialize input and weight buffers\n",
        "  initializeBuffer(input, inputSize);\n",
        "  initializeBuffer(weight, weightSize);\n",
        "\n",
        "  // TODO: Calculate output dimension and allocate output buffer\n",
        "  const unsigned int output_p = input_h - (weight_r - 1) - 1 + 1;\n",
        "  const unsigned int output_q = input_w - (weight_s - 1) - 1 + 1;\n",
        "  const unsigned int output_c = weight_o;\n",
        "  const unsigned int opSize = output_c * output_p * output_q;\n",
        "  float* output = allocateAlignedFloatArray(opSize);\n",
        "  initializeBuffer(output, opSize);\n",
        "\n",
        "  // Calculate memory usage manually\n",
        "  // Assuming each float variable consumes 4 bytes\n",
        "  unsigned int memoryUsageKB = (weightSize + inputSize + opSize) * 4 / 1024;\n",
        "  cout << \"Manual Memory Usage (KB): \" << memoryUsageKB << endl;\n",
        "\n",
        "  // measure memory usage for 3 buffers above only\n",
        "  getMemoryUsage(subsequentVirtualMemoryUsage, subsequentResidentSetSize);\n",
        "  cout << \"Virtual Memory (KB): \"\n",
        "       << subsequentVirtualMemoryUsage - initialVirtualMemoryUsage << endl;\n",
        "\n",
        "  // Flush cache\n",
        "  flushCache(cacheFlush, cacheFloatSize);\n",
        "\n",
        "  auto start = high_resolution_clock::now();\n",
        "  // TODO: Use nested for loops to compute convolution\n",
        "  for (int m = 0; m < output_c; m++) {\n",
        "    for (int q = 0; q < output_q; q++) {\n",
        "      for (int p = 0; p < output_p; p++) {\n",
        "        output[INDEX_4D_TO_1D(0, m, p, q, 0, output_c, output_p, output_q)] = 0.0;\n",
        "        for (int c = 0; c < input_c; c++) {\n",
        "          for (int s = 0; s < weight_s; s++) {\n",
        "            for (int r = 0; r < weight_r; r++) {\n",
        "              output[INDEX_4D_TO_1D(0, m, p, q, 0, output_c, output_p, output_q)] +=\n",
        "                  input[INDEX_4D_TO_1D(0, c, p + r, q + s, 0, input_c, input_h, input_w)] *\n",
        "                  weight[INDEX_4D_TO_1D(m, c, r, s, output_c, input_c, weight_r, weight_s)];\n",
        "            }\n",
        "          }\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "\n",
        "  auto stop = high_resolution_clock::now();\n",
        "  auto duration = duration_cast<milliseconds>(stop - start);\n",
        "  cout << \"Time taken by convolution: \" << duration.count() << \" milliseconds\" << endl;\n",
        "\n",
        "  // Sum all outputs\n",
        "  float sum = 0.0;\n",
        "  for (int i = 0; i < opSize; i++) {\n",
        "    sum += output[i];\n",
        "  }\n",
        "  cout.precision(20);\n",
        "  cout << \"Sum of all outputs: \" << sum << endl;\n",
        "\n",
        "  // Free all allocated buffers\n",
        "  deallocateAlignedFloatArray(input);\n",
        "  cout << \"Input freed\" << endl;\n",
        "  deallocateAlignedFloatArray(weight);\n",
        "  cout << \"Weight freed\" << endl;\n",
        "  deallocateAlignedFloatArray(output);\n",
        "  cout << \"Output freed\" << endl;\n",
        "\n",
        "  return 0;\n",
        "}\n"
      ],
      "metadata": {
        "id": "KrFNRS4n8TON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBXb2fV_-YO7"
      },
      "source": [
        "## Q2\n",
        "Use Img2col algorithm to convert the input matrix and kernel matrix to toeplitz form in file ``q2_conv2d_toeplitz.cc``. Interpret input toeplitz matrix and kernel toeplitz matrix as 2d matrix, and store both of them in row major. Use nested for loops to perform matrix multiplication in `matMul` function and call it in `main` function . There is another micro `INDEX_2D_TO_1D` that helps convert 2d index to 1d index. Measure and report runtime and memory usage. Manually calculate memory usage and report. Does your calculation match with the measurement?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**For rest of the toeplitz questions Buffer size:**\n",
        "\n",
        "Toeplitz Input = 128 * 3 * 3 * 126 * 126\n",
        "\n",
        "Toeplitz Weight = 128 * 128 * 3 * 3\n",
        "\n",
        "Toeplitz Output = 128 * 126 * 126\n",
        "\n",
        "***Memory usage = 79956 KB ***"
      ],
      "metadata": {
        "id": "pC9KeebB62mR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#include <immintrin.h>\n",
        "#include <chrono>\n",
        "#include <iostream>\n",
        "#include \"../utils.h\"\n",
        "\n",
        "using namespace std;\n",
        "using namespace std::chrono;\n",
        "\n",
        "void matMul(const float* A, const float* B, float* C, int A_rows, int A_cols,\n",
        "            int B_rows, int B_cols) {\n",
        "  if (A_cols != B_rows) {\n",
        "    // The matrices can't be multiplied if A's number of columns\n",
        "    // isn't equal to B's number of rows.\n",
        "    throw std::invalid_argument(\n",
        "        \"Matrix dimensions mismatch for multiplication\");\n",
        "  }\n",
        "\n",
        "  // TODO: implement matmul using 3 loops\n",
        "  for (int out_row = 0; out_row < A_rows; out_row++){\n",
        "    for (int out_col = 0; out_col < B_cols; out_col++){\n",
        "      C[INDEX_2D_TO_1D(out_row, out_col, A_rows, B_cols)] = 0.0;\n",
        "      for (int innercnt = 0; innercnt < A_cols; innercnt++) {\n",
        "        C[INDEX_2D_TO_1D(out_row, out_col, A_rows, B_cols)] += A[INDEX_2D_TO_1D(out_row, innercnt, A_rows, A_cols)] * B[INDEX_2D_TO_1D(innercnt, out_col, B_rows, B_cols)];\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "int main(void) {\n",
        "  // modify the cachesize to match your system configuration in Google Colab\n",
        "  const unsigned int cacheSize = 55 * 1024 * 1024;\n",
        "  const unsigned int cacheFloatSize = cacheSize / sizeof(float);\n",
        "\n",
        "  float* cacheFlush = allocateAlignedFloatArray(cacheSize);\n",
        "\n",
        "  // dimension of the convolution weight\n",
        "  const unsigned int weight_o = 128;\n",
        "  const unsigned int weight_i = 128;\n",
        "  const unsigned int weight_r = 3;\n",
        "  const unsigned int weight_s = 3;\n",
        "\n",
        "  // dimension of the convolution input\n",
        "  const unsigned int input_c = 128;\n",
        "  const unsigned int input_h = 128;\n",
        "  const unsigned int input_w = 128;\n",
        "\n",
        "  // Calculate the sizes needed for the arrays\n",
        "  const unsigned int weightSize = weight_o * weight_i * weight_r * weight_s;\n",
        "  const unsigned int inputSize = input_c * input_h * input_w;\n",
        "\n",
        "  // Allocate memory for the arrays using the provided functions\n",
        "  float* weight = allocateAlignedFloatArray(weightSize);\n",
        "  float* input = allocateAlignedFloatArray(inputSize);\n",
        "\n",
        "  // init input and weight buffers\n",
        "  initializeBuffer(input, inputSize);\n",
        "  initializeBuffer(weight, weightSize);\n",
        "\n",
        "  // Declare variables to store the initial memory usage values.\n",
        "  double initialVirtualMemoryUsage, initialResidentSetSize;\n",
        "\n",
        "  // Declare variables to store any subsequent memory usage values (to be used\n",
        "  // later if needed).\n",
        "  double subsequentVirtualMemoryUsage, subsequentResidentSetSize;\n",
        "\n",
        "  // Call the function to get the current memory usage and store the values in\n",
        "  // the initial variables.\n",
        "  getMemoryUsage(initialVirtualMemoryUsage, initialResidentSetSize);\n",
        "\n",
        "  // TODO: calculate dimension of toeplitz output and allocate buffer\n",
        "  const unsigned int output_c = weight_o;\n",
        "  const unsigned int output_p = input_h - weight_r + 1;\n",
        "  const unsigned int output_q = input_w - weight_s + 1;\n",
        "  const unsigned int output_row = weight_o;\n",
        "  const unsigned int output_col = output_p * output_q;\n",
        "  const unsigned int opSize = output_row * output_col;\n",
        "  float* output = allocateAlignedFloatArray(opSize);\n",
        "\n",
        "  // TODO: calculate dimension of toeplitz input and allocate buffer\n",
        "  const unsigned int new_input_h = weight_r * weight_s * weight_i;\n",
        "  const unsigned int new_input_w = output_p * output_q;\n",
        "  const unsigned int new_ipSize = new_input_h * new_input_w;\n",
        "  float* toep_input = allocateAlignedFloatArray(new_ipSize);\n",
        "\n",
        "  // TODO: calculate dimension of toeplitz weight and allocate buffer\n",
        "  const unsigned int new_weight_r = weight_o;\n",
        "  const unsigned int new_weight_s = new_input_h;\n",
        "  const unsigned int new_weightSize = new_weight_r * new_weight_s;\n",
        "  float* toep_weight = allocateAlignedFloatArray(new_weightSize);\n",
        "\n",
        "  // measure memory usage for 3 buffers above only\n",
        "  getMemoryUsage(subsequentVirtualMemoryUsage, subsequentResidentSetSize);\n",
        "  std::cout << \"Virtual Memory (KB): \"\n",
        "            << subsequentVirtualMemoryUsage - initialVirtualMemoryUsage\n",
        "            << std::endl;\n",
        "\n",
        "  // TODO: convert matrix input & weight into toeplitz matrices\n",
        "  // toeplitz input is stored in row major and toeplitz weight is stored in row\n",
        "  // major order\n",
        "  // toeplitz input matrix in row-major\n",
        "  int row = 0;\n",
        "  for (int inc = 0; inc < weight_i; inc++){\n",
        "    for (int r = 0; r < weight_r; r++) {\n",
        "      for (int s = 0; s < weight_s; s++){\n",
        "        int col = 0;\n",
        "        for (int p = 0; p < output_p; p++){\n",
        "          for (int q = 0; q < output_q; q++){\n",
        "            toep_input[INDEX_2D_TO_1D(row, col, new_input_h, new_input_w)] = input[INDEX_4D_TO_1D(0,inc, p + r, q + s, 0, weight_i, input_h, input_w)];\n",
        "            col++;\n",
        "          }\n",
        "        }\n",
        "        row++;\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "\n",
        "  // toeplitz weight matrix in row-major\n",
        "  row = 0;\n",
        "  for (int m = 0; m < new_weight_r; m++){\n",
        "    int col = 0;\n",
        "    for (int inc = 0; inc < weight_i; inc++){\n",
        "      for (int r = 0; r < weight_r; r++) {\n",
        "        for (int s = 0; s < weight_s; s++){\n",
        "          toep_weight[INDEX_2D_TO_1D(row, col, new_weight_r, new_weight_s)] = weight[INDEX_4D_TO_1D(m,inc, r, s, new_weight_r, weight_i, weight_r, weight_s)];\n",
        "          col++;\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "    row++;\n",
        "  }\n",
        "\n",
        "  // Flush cache\n",
        "  flushCache(cacheFlush, cacheFloatSize);\n",
        "\n",
        "  auto start = high_resolution_clock::now();\n",
        "  // TODO: compute matmul between toeplitz matrices, you can create a temp\n",
        "  // buffer to store output_toeplitz measure runtime of this code only\n",
        "  // Allocate memory for temp_output using the provided function\n",
        "  float* temp_output = allocateAlignedFloatArray(opSize);\n",
        "  matMul(toep_weight, toep_input, temp_output, new_weight_r, new_weight_s, new_input_h, new_input_w);\n",
        "\n",
        "  auto stop = high_resolution_clock::now();\n",
        "  auto duration = duration_cast<milliseconds>(stop - start);\n",
        "  cout << \"Time taken by function: \" << duration.count() << \" milliseconds\"\n",
        "       << endl;\n",
        "\n",
        "  // TODO: reformat toeplitz y into row major\n",
        "\n",
        "  //printSumOfOutputs(output, output_c * output_p * output_q);\n",
        "  printSumOfOutputs(temp_output, output_c * output_p * output_q);\n",
        "\n",
        "  // free all allocated buffers\n",
        "  deallocateAlignedFloatArray(input);\n",
        "  deallocateAlignedFloatArray(weight);\n",
        "  deallocateAlignedFloatArray(output);\n",
        "  deallocateAlignedFloatArray(temp_output);\n",
        "  return 0;\n",
        "}\n"
      ],
      "metadata": {
        "id": "vYiad9dc8g-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjmFTWrK_FJU"
      },
      "source": [
        "## Q3\n",
        "For `q3_conv2d_toeplitz_tranB.cc`, repeat procedures in Q2, but store kernel toeplitz matrix in column major and modify `matMul` function accordingly. Measure and report the runtime and memory usage."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#include <immintrin.h>\n",
        "#include <chrono>\n",
        "#include <iostream>\n",
        "#include \"../utils.h\"\n",
        "\n",
        "using namespace std;\n",
        "using namespace std::chrono;\n",
        "\n",
        "void matMulTransB(const float* A, const float* B, float* C, int A_rows, int A_cols,\n",
        "            int B_rows, int B_cols) {\n",
        "  if (A_cols != B_rows) {\n",
        "    // The matrices can't be multiplied if A's number of columns\n",
        "    // isn't equal to B's number of rows.\n",
        "    throw std::invalid_argument(\n",
        "        \"Matrix dimensions mismatch for multiplication\");\n",
        "  }\n",
        "\n",
        "  int cnt = 0;\n",
        "  // TODO: implement matmul using 3 loops\n",
        "  for (int i = 0; i < A_rows; i++){\n",
        "    for (int k = 0; k < B_cols; k++){\n",
        "      for (int j = 0; j < A_cols; j++){\n",
        "        C[cnt] += A[INDEX_2D_TO_1D(j,i,0,A_rows)] * B[INDEX_2D_TO_1D(k,j,0,B_rows)];\n",
        "      }\n",
        "      cnt++;\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "int main(void) {\n",
        "  // modify the cachesize to match your system configuration in Google Colab\n",
        "  const unsigned int cacheSize = 55 * 1024 * 1024;\n",
        "  const unsigned int cacheFloatSize = cacheSize / sizeof(float);\n",
        "\n",
        "  float* cacheFlush = allocateAlignedFloatArray(cacheSize);\n",
        "\n",
        "  // dimension of the convolution weight\n",
        "  const unsigned int weight_o = 128;\n",
        "  const unsigned int weight_i = 128;\n",
        "  const unsigned int weight_r = 3;\n",
        "  const unsigned int weight_s = 3;\n",
        "\n",
        "  // dimension of the convolution input\n",
        "  const unsigned int input_c = 128;\n",
        "  const unsigned int input_h = 128;\n",
        "  const unsigned int input_w = 128;\n",
        "\n",
        "  // Calculate the sizes needed for the arrays\n",
        "  const unsigned int weightSize = weight_o * weight_i * weight_r * weight_s;\n",
        "  const unsigned int inputSize = input_c * input_h * input_w;\n",
        "\n",
        "  // Allocate memory for the arrays using the provided functions\n",
        "  float* weight = allocateAlignedFloatArray(weightSize);\n",
        "  float* input = allocateAlignedFloatArray(inputSize);\n",
        "\n",
        "  // init input and weight buffers\n",
        "  initializeBuffer(input, inputSize);\n",
        "  initializeBuffer(weight, weightSize);\n",
        "\n",
        "  // Declare variables to store the initial memory usage values.\n",
        "  double initialVirtualMemoryUsage, initialResidentSetSize;\n",
        "\n",
        "  // Declare variables to store any subsequent memory usage values (to be used\n",
        "  // later if needed).\n",
        "  double subsequentVirtualMemoryUsage, subsequentResidentSetSize;\n",
        "\n",
        "  // Call the function to get the current memory usage and store the values in\n",
        "  // the initial variables.\n",
        "  getMemoryUsage(initialVirtualMemoryUsage, initialResidentSetSize);\n",
        "\n",
        "  // TODO: calculate dimension of toeplitz output and allocate buffer\n",
        "  const unsigned int output_c = weight_o;\n",
        "  const unsigned int output_p = input_h - weight_r + 1;\n",
        "  const unsigned int output_q = input_w - weight_s + 1;\n",
        "  const unsigned int output_row = weight_o;\n",
        "  const unsigned int output_col = output_p * output_q;\n",
        "  const unsigned int opSize = output_row * output_col;\n",
        "  float* output = allocateAlignedFloatArray(opSize);\n",
        "\n",
        "  // TODO: calculate dimension of toeplitz input and allocate buffer\n",
        "  const unsigned int new_input_h = weight_r * weight_s * weight_i;\n",
        "  const unsigned int new_input_w = output_p * output_q;\n",
        "  const unsigned int new_ipSize = new_input_h * new_input_w;\n",
        "  float* new_input = allocateAlignedFloatArray(new_ipSize);\n",
        "\n",
        "  // TODO: calculate dimension of toeplitz weight and allocate buffer\n",
        "  const unsigned int new_weight_r = weight_o;\n",
        "  const unsigned int new_weight_s = new_input_h;\n",
        "  const unsigned int new_weightSize = new_weight_r * new_weight_s;\n",
        "  float* new_weight = allocateAlignedFloatArray(new_weightSize);\n",
        "\n",
        "  // measure memory usage for 3 buffers above only\n",
        "  getMemoryUsage(subsequentVirtualMemoryUsage, subsequentResidentSetSize);\n",
        "  std::cout << \"Virtual Memory (KB): \"\n",
        "            << subsequentVirtualMemoryUsage - initialVirtualMemoryUsage\n",
        "            << std::endl;\n",
        "\n",
        "  // TODO: convert matrix input & weight into toeplitz matrices\n",
        "  // toeplitz input is stored in row major and toeplitz weight is stored in column\n",
        "  // major order\n",
        "  // input matrix in column-major\n",
        "  int cnt = 0;\n",
        "  int col = 0;\n",
        "  for (int p = 0; p < output_p; p++){\n",
        "    for (int q = 0; q < output_q; q++){\n",
        "      for (int inc = 0; inc < weight_i; inc++){\n",
        "        for (int r = 0; r < weight_r; r++) {\n",
        "          for (int s = 0; s < weight_s; s++){\n",
        "            new_input[cnt] = input[INDEX_4D_TO_1D(0,inc, p + r, q + s, 0, weight_i, input_h, input_w)];\n",
        "            cnt++;\n",
        "          }\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "\n",
        "  // weight matrix in column-major\n",
        "  col = 0;\n",
        "  for (int inc = 0; inc < weight_i; inc++){\n",
        "    for (int r = 0; r < weight_r; r++) {\n",
        "      for (int s = 0; s < weight_s; s++){\n",
        "        for (int m = 0; m < new_weight_r; m++){\n",
        "          new_weight[col] = weight[INDEX_4D_TO_1D(m,inc, r, s, new_weight_r, weight_i, weight_r, weight_s)];\n",
        "          col++;\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "\n",
        "  // Flush cache\n",
        "  flushCache(cacheFlush, cacheFloatSize);\n",
        "\n",
        "  auto start = high_resolution_clock::now();\n",
        "  // TODO: compute matmul between toeplitz matrices, you can create a temp\n",
        "  // buffer to store output_toeplitz measure runtime of this code only\n",
        "  // Allocate memory for temp_output using the provided function\n",
        "  matMulTransB(new_weight, new_input, output, new_weight_r, new_weight_s, new_input_h, new_input_w);\n",
        "\n",
        "  auto stop = high_resolution_clock::now();\n",
        "  auto duration = duration_cast<milliseconds>(stop - start);\n",
        "  cout << \"Time taken by function: \" << duration.count() << \" milliseconds\"\n",
        "       << endl;\n",
        "\n",
        "  // TODO: reformat toeplitz y into row major\n",
        "\n",
        "  printSumOfOutputs(output, output_c * output_p * output_q);\n",
        "\n",
        "  // free all allocated buffers\n",
        "  deallocateAlignedFloatArray(input);\n",
        "  deallocateAlignedFloatArray(weight);\n",
        "  deallocateAlignedFloatArray(output);\n",
        "  //deallocateAlignedFloatArray(temp_output);\n",
        "  return 0;\n",
        "}\n"
      ],
      "metadata": {
        "id": "T7tm-cEl8se0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5SaluD5_ydp"
      },
      "source": [
        "## Q4\n",
        "For ``q4_conv2d_toeplitz_avx.cc``, use Intel AVX (Advanced Vector Extensions) instruction set to perform matmul operation. Intel AVX instructions are Single Instruction Multiple Data (SIMD) instructions that can process 8 floating-point operands in a single instruction. Store input toeplitz matrix in row major and kernel toeplitz matrix in column major. An example of using Intel AVX is given in file ``examples/example_vectorsum_simd.cc``. Measure and report runtime and memory usage.\n",
        "\n",
        "Hint 1: Use `_mm256_add_ps` and `_mm256_mul_ps` instructions.\n",
        "\n",
        "Hint 2: If a vector is not divisible by 8, the remaining elements in the vector should not be processed by SIMD instruction. Instead, use normal scalar operations."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#include <immintrin.h>\n",
        "#include <chrono>\n",
        "#include <iostream>\n",
        "#include \"../utils.h\"\n",
        "\n",
        "using namespace std;\n",
        "using namespace std::chrono;\n",
        "\n",
        "// Assume row-major for A and column-major for B\n",
        "void matMulAVXTransB(const float* A, const float* B, float* C, int A_rows,\n",
        "                     int A_cols, int B_rows, int B_cols) {\n",
        "  if (A_cols != B_rows) {\n",
        "    throw std::invalid_argument(\n",
        "        \"Matrix dimensions mismatch for multiplication\");\n",
        "  }\n",
        "\n",
        "  // TODO: implement AVX version of matMulTransB ///?????????\n",
        "\n",
        "  // Check if A_cols is divisible by 8 for efficient AVX processing\n",
        "  //__m256 sum = _mm256_setzero_ps();\n",
        "    for (int i = 0; i < A_rows; i++) {\n",
        "      for (int j = 0; j < B_cols; j++) {\n",
        "        __m256 sum = _mm256_setzero_ps();\n",
        "        float out[8];\n",
        "        int k;\n",
        "        float temp_out = 0;\n",
        "        for (k = 0; k < A_cols-8; k += 8) {\n",
        "          __m256 a = _mm256_loadu_ps(&A[i * A_cols + k]);\n",
        "          __m256 b = _mm256_loadu_ps(&B[j * B_rows + k]);\n",
        "          sum = _mm256_add_ps(sum, _mm256_mul_ps(a, b));\n",
        "        }\n",
        "        _mm256_storeu_ps(out, sum);\n",
        "        temp_out = out[0] + out[1]+out[2]+out[3]+out[4]+out[5]+out[6]+out[7];\n",
        "        //cout << k;\n",
        "        // check :\n",
        "        float val = temp_out;\n",
        "        //cout << \" here\";\n",
        "        for (; k < A_cols; k++) {\n",
        "              val +=\n",
        "                    A[INDEX_2D_TO_1D(i, k, A_rows, A_cols)] * B[INDEX_2D_TO_1D(j,k, B_cols, A_cols)];\n",
        "        }\n",
        "\n",
        "    C[INDEX_2D_TO_1D(i, j, A_rows, B_cols)]=val;\n",
        "  }\n",
        "}\n",
        "}\n",
        "\n",
        "\n",
        "int main(void) {\n",
        "  // modify the cachesize to match your system configuration in Google Colab\n",
        "  const unsigned int cacheSize = 72 * 1024 * 1024;\n",
        "  const unsigned int cacheFloatSize = cacheSize / sizeof(float);\n",
        "\n",
        "  float* cacheFlush = allocateAlignedFloatArray(cacheSize);\n",
        "\n",
        "  // dimension of the convolution weight\n",
        "  const unsigned int weight_o = 128;\n",
        "  const unsigned int weight_i = 128;\n",
        "  const unsigned int weight_r = 3;\n",
        "  const unsigned int weight_s = 3;\n",
        "\n",
        "  // dimension of the convolution input\n",
        "  const unsigned int input_c = 128;\n",
        "  const unsigned int input_h = 128;\n",
        "  const unsigned int input_w = 128;\n",
        "\n",
        "\n",
        "\n",
        "  // Calculate the sizes needed for the arrays\n",
        "  const unsigned int weightSize = weight_o * weight_i * weight_r * weight_s;\n",
        "  const unsigned int inputSize = input_c * input_h * input_w;\n",
        "\n",
        "  // Allocate memory for the arrays using the provided functions\n",
        "  float* weights = allocateAlignedFloatArray(weightSize);\n",
        "  float* inputs = allocateAlignedFloatArray(inputSize);\n",
        "  //float* output = allocateAlignedFloatArray(output_c*output_p*output_q);\n",
        "\n",
        "  // init input and weight buffers\n",
        "  initializeBuffer(inputs, inputSize);\n",
        "  initializeBuffer(weights, weightSize);\n",
        "\n",
        "  // Declare variables to store the initial memory usage values.\n",
        "  double initialVirtualMemoryUsage, initialResidentSetSize;\n",
        "\n",
        "  // Declare variables to store any subsequent memory usage values (to be used\n",
        "  // later if needed).\n",
        "  double subsequentVirtualMemoryUsage, subsequentResidentSetSize;\n",
        "\n",
        "  // Call the function to get the current memory usage and store the values in\n",
        "  // the initial variables.\n",
        "  getMemoryUsage(initialVirtualMemoryUsage, initialResidentSetSize);\n",
        "\n",
        "  // TODO: calculate dimension of toep output and allocate buffer\n",
        "  const unsigned int output_p = (input_h - weight_r + 1);\n",
        "  const unsigned int output_q = (input_w - weight_s + 1);\n",
        "  const unsigned int output_c = weight_o;\n",
        "  const unsigned int outputToepRows = (input_h - weight_r + 1) * (input_w - weight_s + 1);\n",
        "  const unsigned int outputToepCols =  weight_o;//weight_i * weight_r * weight_s;????\n",
        "  const unsigned int outputToepSize = outputToepRows*outputToepCols;\n",
        "  float* temp_outputs = allocateAlignedFloatArray(outputToepRows * outputToepCols);\n",
        "\n",
        "  // TODO: calculate dimension of toep input and allocate buffer\n",
        "  const unsigned int inputToepRows = (input_h - weight_r + 1) * (input_w - weight_s + 1);\n",
        "  const unsigned int inputToepCols = weight_i * weight_r * weight_s;\n",
        "  float* toepInputs = allocateAlignedFloatArray(inputToepRows * inputToepCols);\n",
        "\n",
        "  // TODO: calculate dimension of toep weight and allocate buffer\n",
        "  const unsigned int weightToepRows = weight_i * weight_r * weight_s;\n",
        "  const unsigned int weightToepCols = weight_o;\n",
        "  float* toepWeights = allocateAlignedFloatArray(weightToepRows * weightToepCols);\n",
        "\n",
        "  // measure memory usage for 3 buffers above only\n",
        "  getMemoryUsage(subsequentVirtualMemoryUsage, subsequentResidentSetSize);\n",
        "  std::cout << \"Virtual Memory (KB): \"\n",
        "            << subsequentVirtualMemoryUsage - initialVirtualMemoryUsage\n",
        "            << std::endl;\n",
        "\n",
        "  // TODO: convert matrix input & weight into toep matrices\n",
        "  // toep input is stored in row major and toep weight is stored in column\n",
        "  // major order\n",
        "  // Convert input matrix to Toeplitz format (row major)\n",
        "// Compute Toeplitz input matrix\n",
        "// Compute Toeplitz input matrix\n",
        "for (int i = 0; i < input_h - weight_r + 1; i++) {\n",
        "    for (int j = 0; j < input_w - weight_s + 1; j++) {\n",
        "        for (int c = 0; c < input_c; c++) {\n",
        "            for (int r = 0; r < weight_r; r++) {\n",
        "                for (int s = 0; s < weight_s; s++) {\n",
        "                    //unsigned int row_idx = i * (input_w - weight_s + 1) + j;\n",
        "                    //unsigned int col_idx = (c * weight_r * weight_s) + (r * weight_s) + s;\n",
        "                    toepInputs[INDEX_2D_TO_1D(i * (input_w - weight_s + 1) + j,(c * weight_r * weight_s) + (r * weight_s) + s,inputToepRows,inputToepCols)]=\n",
        "                       inputs[INDEX_4D_TO_1D(0, c, (i + r),(j + s),1, input_c, input_h,input_w)];\n",
        "\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n",
        "// Compute Toeplitz weight matrix\n",
        "\n",
        "\n",
        "    for (int c = 0; c < weight_i; c++) {\n",
        "        for (int r = 0; r < weight_r; r++) {\n",
        "            for (int s = 0; s < weight_s; s++) {\n",
        "                for (int o = 0; o < weight_o; o++)\n",
        "\n",
        "                toepWeights[INDEX_2D_TO_1D(o,c * weight_r * weight_s + r * weight_s + s,weightToepCols, weightToepRows)] =\n",
        "                        weights[INDEX_4D_TO_1D(o,c,r, s, weight_o, weight_i, weight_r, weight_s)];\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "  // Flush cache\n",
        "  flushCache(cacheFlush, cacheFloatSize);\n",
        "  //float* temp_outputs = allocateAlignedFloatArray(outputToepSize);\n",
        "  auto start = high_resolution_clock::now();\n",
        "\n",
        "\n",
        "  // TODO: compute matmul between toep matrices, you can create a temp\n",
        "  // buffer to store output_toep measure runtime of this code only\n",
        "  // Allocate memory for temp_output using the provided function\n",
        "\n",
        "  matMulAVXTransB(toepInputs,toepWeights, temp_outputs,  inputToepRows, inputToepCols, weightToepRows, weightToepCols);\n",
        "\n",
        "  auto stop = high_resolution_clock::now();\n",
        "  auto duration = duration_cast<milliseconds>(stop - start);\n",
        "  std::cout << \"Time taken by function: \" << duration.count() << \" milliseconds\"\n",
        "       << endl;\n",
        "\n",
        "  // TODO: reformat toep y into col major\n",
        "  float* outputs = allocateAlignedFloatArray(outputToepRows * outputToepCols);\n",
        "  for (unsigned int i = 0; i < outputToepRows; ++i) {\n",
        "    for (unsigned int j = 0; j < outputToepCols; ++j) {\n",
        "        outputs[j * outputToepRows + i] = temp_outputs[i * outputToepCols + j];\n",
        "    }\n",
        "}\n",
        "\n",
        "  printSumOfOutputs(outputs, output_c * output_p * output_q);\n",
        "\n",
        "  // free all allocated buffers\n",
        "  deallocateAlignedFloatArray(cacheFlush);\n",
        "  deallocateAlignedFloatArray(inputs);\n",
        "  deallocateAlignedFloatArray(weights);\n",
        "  deallocateAlignedFloatArray(outputs);\n",
        "  deallocateAlignedFloatArray(temp_outputs);\n",
        "  deallocateAlignedFloatArray(toepWeights);\n",
        "  deallocateAlignedFloatArray(toepInputs);\n",
        "  return 0;\n",
        "}\n",
        "\n"
      ],
      "metadata": {
        "id": "3FQxu4wv8xih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqRyp0x8A99J"
      },
      "source": [
        "## Q5\n",
        "\n",
        "### Part 1\n",
        "In file ``q5_conv2d_toeplitz_avx_openmp.cc``, futher optimize the AVX matmul operation using ``OpenMP`` library, which enables multi-threaded parallel computing automatically through a simple and flexible interface. An example of using ``OpenMP`` is given in file ``examples/example_vectorsum_simd_omd.cc``. Run the code with different number of threads. Measure and report the runtime for number of threads = [2, 4, 8].\n",
        "\n",
        "\n",
        "### Part 2 (Optional - No Credit)\n",
        "\n",
        "In file ``q5_optional_conv2d_toeplitz_avx_multi_thread.cc``, instead of using `OpenMP`, use the C++ standard `<thread>` library to implement multi-threaded AVX matmul operation. You can refer to [this video](https://youtu.be/3aqxaZsvn80?si=1QEE580e2vLmrqPO) to learn about multi threading in C++.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#include <immintrin.h>\n",
        "#include <omp.h>\n",
        "#include <chrono>\n",
        "#include <iostream>\n",
        "#include <vector>\n",
        "#include \"../utils.h\"\n",
        "\n",
        "using namespace std;\n",
        "using namespace std::chrono;\n",
        "\n",
        "// Assume row-major for A and column-major for B\n",
        "void matMulAVXOpenMPTransB(const float* A, const float* B, float* C, int A_rows,\n",
        "                           int A_cols, int B_rows, int B_cols) {\n",
        "  if (A_cols != B_rows) {\n",
        "    throw std::invalid_argument(\n",
        "        \"Matrix dimensions mismatch for multiplication\");\n",
        "  }\n",
        "\n",
        "  // TODO: implement AVX version of matMulTransB with OpenMP\n",
        "  // Parallelize the matrix multiplication using OpenMP\n",
        "  #pragma omp parallel for shared(A, B, C) collapse(2)\n",
        "  for (int i = 0; i < A_rows; i++) {\n",
        "    for (int j = 0; j < B_cols; j++) {\n",
        "      __m256 sum = _mm256_setzero_ps();\n",
        "      float out[8];\n",
        "      int k;\n",
        "      float temp_out = 0;\n",
        "      for (k = 0; k < A_cols - 8; k += 8) {\n",
        "        __m256 a = _mm256_loadu_ps(&A[i * A_cols + k]);\n",
        "        __m256 b = _mm256_loadu_ps(&B[j * B_rows + k]);\n",
        "        sum = _mm256_add_ps(sum, _mm256_mul_ps(a, b));\n",
        "      }\n",
        "      _mm256_storeu_ps(out, sum);\n",
        "      temp_out = out[0] + out[1] + out[2] + out[3] + out[4] + out[5] + out[6] + out[7];\n",
        "\n",
        "      float val = temp_out;\n",
        "\n",
        "      for (; k < A_cols; k++) {\n",
        "        val += A[INDEX_2D_TO_1D(i, k, A_rows, A_cols)] * B[INDEX_2D_TO_1D(j, k, B_cols, A_cols)];\n",
        "      }\n",
        "\n",
        "      C[INDEX_2D_TO_1D(i, j, A_rows, B_cols)] = val;\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "int main(void) {\n",
        "  // modify the cachesize to match your system configuration in Google Colab\n",
        "  const unsigned int cacheSize = 55 * 1024 * 1024;\n",
        "  const unsigned int cacheFloatSize = cacheSize / sizeof(float);\n",
        "\n",
        "  float* cacheFlush = allocateAlignedFloatArray(cacheSize);\n",
        "\n",
        "  // dimension of the convolution weight\n",
        "  const unsigned int weight_o = 128;\n",
        "  const unsigned int weight_i = 128;\n",
        "  const unsigned int weight_r = 3;\n",
        "  const unsigned int weight_s = 3;\n",
        "\n",
        "  // dimension of the convolution input\n",
        "  const unsigned int input_c = 128;\n",
        "  const unsigned int input_h = 128;\n",
        "  const unsigned int input_w = 128;\n",
        "\n",
        "  // Calculate the sizes needed for the arrays\n",
        "  const unsigned int weightSize = weight_o * weight_i * weight_r * weight_s;\n",
        "  const unsigned int inputSize = input_c * input_h * input_w;\n",
        "\n",
        "  // Allocate memory for the arrays using the provided functions\n",
        "  float* weight = allocateAlignedFloatArray(weightSize);\n",
        "  float* input = allocateAlignedFloatArray(inputSize);\n",
        "\n",
        "  // init input and weight buffers\n",
        "  initializeBuffer(input, inputSize);\n",
        "  initializeBuffer(weight, weightSize);\n",
        "\n",
        "  // Declare variables to store the initial memory usage values.\n",
        "  double initialVirtualMemoryUsage, initialResidentSetSize;\n",
        "\n",
        "  // Declare variables to store any subsequent memory usage values (to be used\n",
        "  // later if needed).\n",
        "  double subsequentVirtualMemoryUsage, subsequentResidentSetSize;\n",
        "\n",
        "  // Call the function to get the current memory usage and store the values in\n",
        "  // the initial variables.\n",
        "  getMemoryUsage(initialVirtualMemoryUsage, initialResidentSetSize);\n",
        "\n",
        "  // TODO: calculate dimension of toeplitz output and allocate buffer\n",
        "  const unsigned int output_p = (input_h - weight_r + 1);\n",
        "  const unsigned int output_q = (input_w - weight_s + 1);\n",
        "  const unsigned int output_c = weight_o;\n",
        "  const unsigned int outputToepRows = (input_h - weight_r + 1) * (input_w - weight_s + 1);\n",
        "  const unsigned int outputToepCols = weight_o;\n",
        "  const unsigned int outputToepSize = outputToepRows * outputToepCols;\n",
        "  float* temp_outputs = allocateAlignedFloatArray(outputToepRows * outputToepCols);\n",
        "\n",
        "  // TODO: calculate dimension of toeplitz input and allocate buffer\n",
        "  const unsigned int inputToepRows = (input_h - weight_r + 1) * (input_w - weight_s + 1);\n",
        "  const unsigned int inputToepCols = weight_i * weight_r * weight_s;\n",
        "  float* toepInputs = allocateAlignedFloatArray(inputToepRows * inputToepCols);\n",
        "\n",
        "  // TODO: calculate dimension of toeplitz weight and allocate buffer\n",
        "  const unsigned int weightToepRows = weight_i * weight_r * weight_s;\n",
        "  const unsigned int weightToepCols = weight_o;\n",
        "  float* toepWeights = allocateAlignedFloatArray(weightToepRows * weightToepCols);\n",
        "\n",
        "  // measure memory usage for 3 buffers above only\n",
        "  getMemoryUsage(subsequentVirtualMemoryUsage, subsequentResidentSetSize);\n",
        "  std::cout << \"Virtual Memory (KB): \"\n",
        "            << subsequentVirtualMemoryUsage - initialVirtualMemoryUsage\n",
        "            << std::endl;\n",
        "\n",
        "  // TODO: convert matrix input & weight into toeplitz matrices\n",
        "  // toeplitz input is stored in row major and toeplitz weight is stored in\n",
        "  // column major order\n",
        "  // Compute Toeplitz input matrix\n",
        "  for (int i = 0; i < input_h - weight_r + 1; i++) {\n",
        "    for (int j = 0; j < input_w - weight_s + 1; j++) {\n",
        "      for (int c = 0; c < input_c; c++) {\n",
        "        for (int r = 0; r < weight_r; r++) {\n",
        "          for (int s = 0; s < weight_s; s++) {\n",
        "            toepInputs[INDEX_2D_TO_1D(i * (input_w - weight_s + 1) + j,\n",
        "                                      (c * weight_r * weight_s) + (r * weight_s) + s,\n",
        "                                      inputToepRows, inputToepCols)] =\n",
        "                input[INDEX_4D_TO_1D(0, c, (i + r), (j + s), 1, input_c, input_h, input_w)];\n",
        "          }\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "\n",
        "  // Compute Toeplitz weight matrix\n",
        "  for (int c = 0; c < weight_i; c++) {\n",
        "    for (int r = 0; r < weight_r; r++) {\n",
        "      for (int s = 0; s < weight_s; s++) {\n",
        "        for (int o = 0; o < weight_o; o++) {\n",
        "          toepWeights[INDEX_2D_TO_1D(o, c * weight_r * weight_s + r * weight_s + s,\n",
        "                                      weightToepCols, weightToepRows)] =\n",
        "              weight[INDEX_4D_TO_1D(o, c, r, s, weight_o, weight_i, weight_r, weight_s)];\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "\n",
        "  // Flush cache\n",
        "  flushCache(cacheFlush, cacheFloatSize);\n",
        "\n",
        "  int num_threads[] = {2, 4, 8}; // Test with 2, 4, and 8 threads\n",
        "\n",
        "  for (int i = 0; i < 3; ++i) {\n",
        "    omp_set_num_threads(num_threads[i]); // Set the number of threads\n",
        "    auto start = high_resolution_clock::now();\n",
        "    matMulAVXOpenMPTransB(toepInputs, toepWeights, temp_outputs, inputToepRows, inputToepCols, weightToepRows, weightToepCols);\n",
        "\n",
        "    auto stop = high_resolution_clock::now();\n",
        "    auto duration = duration_cast<milliseconds>(stop - start);\n",
        "    cout << \"Time taken by function with \" << num_threads[i] << \" threads: \" << duration.count() << \" milliseconds\" << endl;\n",
        "\n",
        "    // TODO: reformat toeplitz y into row major\n",
        "    float* outputs = allocateAlignedFloatArray(outputToepRows * outputToepCols);\n",
        "    for (unsigned int i = 0; i < outputToepRows; ++i) {\n",
        "      for (unsigned int j = 0; j < outputToepCols; ++j) {\n",
        "        outputs[j * outputToepRows + i] = temp_outputs[i * outputToepCols + j];\n",
        "      }\n",
        "    }\n",
        "\n",
        "    printSumOfOutputs(outputs, output_c * output_p * output_q);\n",
        "    deallocateAlignedFloatArray(outputs);\n",
        "  }\n",
        "\n",
        "  // free all allocated buffers\n",
        "  deallocateAlignedFloatArray(cacheFlush);\n",
        "  deallocateAlignedFloatArray(input);\n",
        "  deallocateAlignedFloatArray(weight);\n",
        "  deallocateAlignedFloatArray(temp_outputs);\n",
        "  deallocateAlignedFloatArray(toepWeights);\n",
        "  deallocateAlignedFloatArray(toepInputs);\n",
        "\n",
        "  return 0;\n",
        "}\n"
      ],
      "metadata": {
        "id": "OGnq1TpR84eL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KW-_R8gm6BUQ"
      },
      "source": [
        "## Q6\n",
        "\n",
        "In file ``q6_conv2d_toeplitz_blas.cc``, use `BLAS` library to implement matmul operation. You can decide the storage format for input and kernel toeplitz matrices, and make sure you set input arguments of `cblas_sgemm` accordingly. An example is given in file ``examples/example_matmul_blas.cc``. Search online document of `cblas_sgemm` API if you are unclear about the mearning of each of its input argument. Follow the example and and finish your own code. Measure and report runtime and memory usage."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#include <immintrin.h>\n",
        "\n",
        "#include <chrono>\n",
        "#include <iostream>\n",
        "\n",
        "#include \"../utils.h\"\n",
        "extern \"C\" {\n",
        "#include \"../cblas.h\"\n",
        "}\n",
        "using namespace std;\n",
        "using namespace std::chrono;\n",
        "\n",
        "void matMulBLAS(const float* A, const float* B, float* C, int A_rows,\n",
        "                int A_cols, int B_rows, int B_cols) {\n",
        "  if (A_cols != B_rows) {\n",
        "    throw std::invalid_argument(\n",
        "        \"Matrix dimensions mismatch for multiplication\");\n",
        "  }\n",
        "\n",
        "  //TODO: call cblas_sgemm to perform matrix multiplication\n",
        "  // Call cblas_sgemm to perform matrix multiplication\n",
        "  cblas_sgemm(CblasRowMajor, CblasNoTrans, CblasNoTrans, A_rows, B_cols,\n",
        "              A_cols, 1.0f, A, A_cols, B, B_cols, 0.0f, C, B_cols);\n",
        "}\n",
        "\n",
        "int main(void) {\n",
        "  // modify the cachesize to match your system configuration in Google Colab\n",
        "  const unsigned int cacheSize = 55 * 1024 * 1024;\n",
        "  const unsigned int cacheFloatSize = cacheSize / sizeof(float);\n",
        "\n",
        "  float* cacheFlush = allocateAlignedFloatArray(cacheSize);\n",
        "\n",
        "  // dimension of the convolution weight\n",
        "  const unsigned int weight_o = 128;\n",
        "  const unsigned int weight_i = 128;\n",
        "  const unsigned int weight_r = 3;\n",
        "  const unsigned int weight_s = 3;\n",
        "\n",
        "  // dimension of the convolution input\n",
        "  const unsigned int input_c = 128;\n",
        "  const unsigned int input_h = 128;\n",
        "  const unsigned int input_w = 128;\n",
        "\n",
        "  // Calculate the sizes needed for the arrays\n",
        "  const unsigned int weightSize = weight_o * weight_i * weight_r * weight_s;\n",
        "  const unsigned int inputSize = input_c * input_h * input_w;\n",
        "\n",
        "  // Allocate memory for the arrays using the provided functions\n",
        "  float* weight = allocateAlignedFloatArray(weightSize);\n",
        "  float* input = allocateAlignedFloatArray(inputSize);\n",
        "\n",
        "  // init input and weight buffers\n",
        "  initializeBuffer(input, inputSize);\n",
        "  initializeBuffer(weight, weightSize);\n",
        "\n",
        "  // Declare variables to store the initial memory usage values.\n",
        "  double initialVirtualMemoryUsage, initialResidentSetSize;\n",
        "\n",
        "  // Declare variables to store any subsequent memory usage values (to be used\n",
        "  // later if needed).\n",
        "  double subsequentVirtualMemoryUsage, subsequentResidentSetSize;\n",
        "\n",
        "  // Call the function to get the current memory usage and store the values in\n",
        "  // the initial variables.\n",
        "  getMemoryUsage(initialVirtualMemoryUsage, initialResidentSetSize);\n",
        "\n",
        "  // TODO: calculate dimension of toep output and allocate buffer\n",
        "  const unsigned int outputToepRows = weight_o;\n",
        "  const unsigned int outputToepCols = (input_h - weight_r + 1) * (input_w - weight_s + 1);\n",
        "  const unsigned int outputToepSize = outputToepRows * outputToepCols;\n",
        "  float* temp_output = allocateAlignedFloatArray(outputToepRows * outputToepCols);\n",
        "\n",
        "  // TODO: calculate dimension of toep input and allocate buffer\n",
        "  const unsigned int inputToepRows = weight_i * weight_r * weight_s;\n",
        "  const unsigned int inputToepCols = (input_h - weight_r + 1) * (input_w - weight_s + 1);\n",
        "  float* toepInput = allocateAlignedFloatArray(inputToepRows * inputToepCols);\n",
        "\n",
        "  // TODO: calculate dimension of toep weight and allocate buffer\n",
        "  const unsigned int weightToepRows = weight_o;\n",
        "  const unsigned int weightToepCols = weight_i * weight_r * weight_s;\n",
        "  float* toepWeight = allocateAlignedFloatArray(weightToepRows * weightToepCols);\n",
        "\n",
        "  // measure memory usage for 3 buffers above only\n",
        "  getMemoryUsage(subsequentVirtualMemoryUsage, subsequentResidentSetSize);\n",
        "  std::cout << \"Virtual Memory (KB): \"\n",
        "            << subsequentVirtualMemoryUsage - initialVirtualMemoryUsage\n",
        "            << std::endl;\n",
        "\n",
        "  // TODO: convert matrix input & weight into toep matrices\n",
        "  // Compute Toep input matrix\n",
        "  int toepRow = 0;\n",
        "  for (unsigned int c = 0; c < input_c; ++c) {\n",
        "    for (unsigned int r = 0; r < weight_r; ++r) {\n",
        "      for (unsigned int s = 0; s < weight_s; ++s) {\n",
        "        int toepCol = 0;\n",
        "        for (unsigned int i = 0; i < input_h - weight_r + 1; ++i) {\n",
        "          for (unsigned int j = 0; j < input_w - weight_s + 1; ++j) {\n",
        "            // Compute Toep indices\n",
        "            toepInput[INDEX_2D_TO_1D(toepRow, toepCol, inputToepRows, inputToepCols)] =\n",
        "                input[INDEX_4D_TO_1D(0, c, i + r, j + s, 0, weight_i, input_h, input_w)];\n",
        "            toepCol++;\n",
        "          }\n",
        "        }\n",
        "        toepRow++;\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "\n",
        "  // Compute Toep weight matrix\n",
        "  toepRow = 0;\n",
        "  for (unsigned int o = 0; o < weight_o; ++o) {\n",
        "    int toepCol = 0;\n",
        "    for (unsigned int c = 0; c < weight_i; ++c) {\n",
        "      for (unsigned int r = 0; r < weight_r; ++r) {\n",
        "        for (unsigned int s = 0; s < weight_s; ++s) {\n",
        "          toepWeight[INDEX_2D_TO_1D(toepRow, toepCol, weightToepRows, weightToepCols)] =\n",
        "              weight[INDEX_4D_TO_1D(o, c, r, s, weight_o, weight_i, weight_r, weight_s)];\n",
        "          toepCol++;\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "    toepRow++;\n",
        "  }\n",
        "\n",
        "  // Flush cache\n",
        "  flushCache(cacheFlush, cacheFloatSize);\n",
        "\n",
        "  auto start = high_resolution_clock::now();\n",
        "  // TODO: compute matmul between toep matrices, you can create a temp\n",
        "  // buffer to store output_toep measure runtime of this code only\n",
        "  // Allocate memory for temp_output using the provided function\n",
        "  matMulBLAS(toepWeight, toepInput, temp_output, weightToepRows, weightToepCols, inputToepRows, inputToepCols);\n",
        "\n",
        "  auto stop = high_resolution_clock::now();\n",
        "  auto duration = duration_cast<milliseconds>(stop - start);\n",
        "  cout << \"Time taken by function: \" << duration.count() << \" milliseconds\"\n",
        "       << endl;\n",
        "\n",
        "  // TODO: reformat toep y into row major\n",
        "  float* output = allocateAlignedFloatArray(outputToepRows * outputToepCols);\n",
        "  for (unsigned int i = 0; i < outputToepRows; ++i) {\n",
        "    for (unsigned int j = 0; j < outputToepCols; ++j) {\n",
        "      output[i * outputToepCols + j] = temp_output[j * outputToepRows + i];\n",
        "    }\n",
        "  }\n",
        "  const unsigned int output_p = (input_h - weight_r + 1);\n",
        "  const unsigned int output_q = (input_w - weight_s + 1);\n",
        "  const unsigned int output_c = weight_o;\n",
        "  printSumOfOutputs(temp_output, output_c * output_p * output_q);\n",
        "\n",
        "  // free all allocated buffers\n",
        "  deallocateAlignedFloatArray(cacheFlush);\n",
        "  deallocateAlignedFloatArray(input);\n",
        "  deallocateAlignedFloatArray(weight);\n",
        "  deallocateAlignedFloatArray(output);\n",
        "  deallocateAlignedFloatArray(temp_output);\n",
        "  deallocateAlignedFloatArray(toepInput);\n",
        "  deallocateAlignedFloatArray(toepWeight);\n",
        "  return 0;\n",
        "}\n"
      ],
      "metadata": {
        "id": "L85cixpH89eN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CVxmA8VB2n5"
      },
      "source": [
        "## Q7  \n",
        "\n",
        "### Part 1\n",
        "\n",
        "\n",
        "Analyze the performance differences between Q1-Q6. Explain what constitutes the performance difference between each implementations and why `BLAS` library is super fast.\n",
        "\n",
        "### Part 2: Using Google Benchmark (Optional - No Credit)\n",
        "As an optional practice, you can measure the runtime of each matrix calculation method more accurately using Google benchmark.\n",
        "\n",
        "A better way to measure performance of a funcion in C++ is to use Google Benchmark. You can refer to [this video](https://youtu.be/9VKR8u9odrA?si=xSInuzT5uMBOKAbP) to familiarize yourself with this package.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-m7NCnmR6BUQ"
      },
      "source": [
        "## Upload files to GitHub\n",
        "Make sure upload your final C++ code and this IPython notebook to GitHub Repo either mannully or through git commands."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}